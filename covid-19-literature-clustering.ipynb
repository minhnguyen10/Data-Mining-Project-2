{"cells":[{"cell_type":"markdown","metadata":{"id":"_Fm-0HT4tq87"},"source":["# COVID-19 Literature Clustering"]},{"cell_type":"markdown","metadata":{"id":"-qnEs9WNtq89"},"source":["\n","\n","### Goal\n","Given a large amount of literature and rapidly spreading COVID-19, it is difficult for a scientist to keep up with the research community promptly. Can we cluster similar research articles together to make it easier for health professionals to find relevant research articles? Clustering can be used to create a tool to identify similar articles, given a target article. It can also reduce the number of articles one has to go through as one can focus on a cluster of articles rather than all. \n","\n","**Approach**:\n","<ol>\n","    <li>Unsupervised Learning task, because we don't have labels for the articles</li>\n","    <li>Clustering and Dimensionality Reduction task </li>\n","    <li>See how well labels from K-Means classify</li>\n","    <li>Use N-Grams with Hash Vectorizer</li>\n","    <li>Use plain text with Tf-idf</li>\n","    <li>Use K-Means for clustering</li>\n","    <li>Use t-SNE for dimensionality reduction</li>\n","    <li>Use PCA for dimensionality reduction</li>\n","    <li>There is no continuous flow of data, no need to adjust to changing data, and the data is small enough to fit in memmory: Batch Learning</li>\n","    <li>Altough, there is no continuous flow of data, our approach has to be scalable as there will be more literature later</li>\n","</ol>\n","\n","### Dataset Description\n","\n",">*In response to the COVID-19 pandemic, the White House and a coalition of leading research groups have prepared the COVID-19 Open Research Dataset (CORD-19). CORD-19 is a resource of over 29,000 scholarly articles, including over 13,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. This freely available dataset is provided to the global research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this infectious disease. There is a growing urgency for these approaches because of the rapid acceleration in new coronavirus literature, making it difficult for the medical research community to keep up.*\n","#### Cite: [COVID-19 Open Research Dataset Challenge (CORD-19) | Kaggle](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) <br>\n"]},{"cell_type":"markdown","metadata":{"id":"Nx34F_Egtq8_"},"source":["# Load the Data\n","Load the data following the notebook by Ivan Ega Pratama, from Kaggle.\n","#### Cite: [Dataset Parsing Code | Kaggle, COVID EDA: Initial Exploration Tool](https://www.kaggle.com/ivanegapratama/covid-eda-initial-exploration-tool)"]},{"cell_type":"markdown","metadata":{"id":"Jg-ChYWVtq8_"},"source":["### Loading Metadata"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"ZAsGyaxPtq8_","executionInfo":{"status":"ok","timestamp":1642887460695,"user_tz":360,"elapsed":155,"user":{"displayName":"Minh Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05642437067105785776"}}},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import glob\n","import json\n","\n","import matplotlib.pyplot as plt\n","plt.style.use('ggplot')"]},{"cell_type":"markdown","metadata":{"id":"9EtLwnd-tq9A"},"source":["Let's load the metadata of the dateset. 'title' and 'journal' attributes may be useful later when we cluster the articles to see what kinds of articles cluster together."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":467},"id":"C9C-Tg9jtq9B","executionInfo":{"status":"error","timestamp":1642887659124,"user_tz":360,"elapsed":218,"user":{"displayName":"Minh Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05642437067105785776"}},"outputId":"26a7f9a2-ef9c-46e8-b07f-92c71b7e1c6b"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-0a4b7fa27056>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'pubmed_id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'Microsoft Academic Paper ID'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m'doi'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m })\n\u001b[1;32m      8\u001b[0m \u001b[0mmeta_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Proj2_Files_F21/Experiment - 4/CORD-19-research-challenge/metadata.csv'"]}],"source":["root_path = 'Proj2_Files_F21/Experiment - 4/CORD-19-research-challenge'\n","metadata_path = f'{root_path}/metadata.csv'\n","meta_df = pd.read_csv(metadata_path, dtype={\n","    'pubmed_id': str,\n","    'Microsoft Academic Paper ID': str, \n","    'doi': str\n","})\n","meta_df.head()"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g9ydEJ8zufmw","executionInfo":{"status":"ok","timestamp":1642887536465,"user_tz":360,"elapsed":20874,"user":{"displayName":"Minh Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05642437067105785776"}},"outputId":"8df98616-0d8a-4f67-f9a5-c17701363588"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TncSANqHtq9B","executionInfo":{"status":"aborted","timestamp":1642887461065,"user_tz":360,"elapsed":10,"user":{"displayName":"Minh Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05642437067105785776"}}},"outputs":[],"source":["meta_df.info()"]},{"cell_type":"markdown","metadata":{"id":"b9kqJjU5tq9C"},"source":["### Fetch All of JSON File Path"]},{"cell_type":"markdown","metadata":{"id":"MzjnM-eBtq9C"},"source":["Get path to all JSON files:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3zDqhpO5tq9C","executionInfo":{"status":"aborted","timestamp":1642887461065,"user_tz":360,"elapsed":10,"user":{"displayName":"Minh Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05642437067105785776"}}},"outputs":[],"source":["all_json = glob.glob(f'{root_path}/**/*.json', recursive=True)\n","len(all_json)"]},{"cell_type":"markdown","metadata":{"id":"bYU5tNGItq9D"},"source":["### Helper Functions"]},{"cell_type":"markdown","metadata":{"id":"PHhQROeDtq9D"},"source":[" File Reader Class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-oBCJ6n6tq9D","executionInfo":{"status":"aborted","timestamp":1642887461066,"user_tz":360,"elapsed":10,"user":{"displayName":"Minh Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05642437067105785776"}}},"outputs":[],"source":["class FileReader:\n","    def __init__(self, file_path):\n","        with open(file_path) as file:\n","            content = json.load(file)\n","            self.paper_id = content['paper_id']\n","            self.abstract = []\n","            self.body_text = []\n","            # Abstract\n","            for entry in content['abstract']:\n","                self.abstract.append(entry['text'])\n","            # Body text\n","            for entry in content['body_text']:\n","                self.body_text.append(entry['text'])\n","            self.abstract = '\\n'.join(self.abstract)\n","            self.body_text = '\\n'.join(self.body_text)\n","    def __repr__(self):\n","        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\n","    \n","# Helper function adds break after every words when character length reach to certain amount. This is for the interactive plot so that hover tool fits the screen.\n","    \n","def get_breaks(content, length):\n","    data = \"\"\n","    words = content.split(' ')\n","    total_chars = 0\n","\n","    # add break every length characters\n","    for i in range(len(words)):\n","        total_chars += len(words[i])\n","        if total_chars > length:\n","            data = data + \"<br>\" + words[i]\n","            total_chars = 0\n","        else:\n","            data = data + \" \" + words[i]\n","    return data\n"]},{"cell_type":"markdown","metadata":{"id":"SiKxzv34tq9D"},"source":["### Load the Data into DataFrame"]},{"cell_type":"markdown","metadata":{"id":"DU6yg25Htq9E"},"source":["Using the helper functions, let's read in the articles into a DataFrame that can be used easily:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X7gVXV5Xtq9E","executionInfo":{"status":"aborted","timestamp":1642887461066,"user_tz":360,"elapsed":10,"user":{"displayName":"Minh Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05642437067105785776"}}},"outputs":[],"source":["dict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\n","for idx, entry in enumerate(all_json):\n","    try:\n","        if idx % (len(all_json) // 10) == 0:\n","            print(f'Processing index: {idx} of {len(all_json)}')\n","        content = FileReader(entry)\n","\n","        # get metadata information\n","        meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n","        # no metadata, skip this paper\n","        if len(meta_data) == 0:\n","            continue\n","\n","        dict_['paper_id'].append(content.paper_id)\n","        dict_['abstract'].append(content.abstract)\n","        dict_['body_text'].append(content.body_text)\n","\n","        # also create a column for the summary of abstract to be used in a plot\n","        if len(content.abstract) == 0: \n","            # no abstract provided\n","            dict_['abstract_summary'].append(\"Not provided.\")\n","        elif len(content.abstract.split(' ')) > 100:\n","            # abstract provided is too long for plot, take first 300 words append with ...\n","            info = content.abstract.split(' ')[:100]\n","            summary = get_breaks(' '.join(info), 40)\n","            dict_['abstract_summary'].append(summary + \"...\")\n","        else:\n","            # abstract is short enough\n","            summary = get_breaks(content.abstract, 40)\n","            dict_['abstract_summary'].append(summary)\n","\n","        # get metadata information\n","        meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n","\n","        try:\n","            # if more than one author\n","            authors = meta_data['authors'].values[0].split(';')\n","            if len(authors) > 2:\n","                # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n","                dict_['authors'].append(\". \".join(authors[:2]) + \"...\")\n","            else:\n","                # authors will fit in plot\n","                dict_['authors'].append(\". \".join(authors))\n","        except Exception as e:\n","            # if only one author - or Null valie\n","            dict_['authors'].append(meta_data['authors'].values[0])\n","\n","        # add the title information, add breaks when needed\n","        try:\n","            title = get_breaks(meta_data['title'].values[0], 40)\n","            dict_['title'].append(title)\n","        # if title was not provided\n","        except Exception as e:\n","            dict_['title'].append(meta_data['title'].values[0])\n","\n","        # add the journal information\n","        dict_['journal'].append(meta_data['journal'].values[0])\n","        \n","    \n","    except Exception as e:\n","        continue\n","    \n","df_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\n","df_covid.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7onJVT9Qtq9E","executionInfo":{"status":"aborted","timestamp":1642887461066,"user_tz":360,"elapsed":10,"user":{"displayName":"Minh Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05642437067105785776"}}},"outputs":[],"source":["dict_ = None"]},{"cell_type":"markdown","metadata":{"id":"bkJbaQyqtq9E"},"source":["# Adding the Word Count Columns"]},{"cell_type":"markdown","metadata":{"id":"I4g6GQbEtq9F"},"source":["Adding word count columns for both abstract and body_text can be useful parameters later:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9sSPCy6Otq9F","executionInfo":{"status":"aborted","timestamp":1642887461067,"user_tz":360,"elapsed":11,"user":{"displayName":"Minh Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05642437067105785776"}}},"outputs":[],"source":["df_covid['abstract_word_count'] = df_covid['abstract'].apply(lambda x: len(x.strip().split()))\n","df_covid['body_word_count'] = df_covid['body_text'].apply(lambda x: len(x.strip().split()))\n","df_covid.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5VyAPZi7tq9F","executionInfo":{"status":"aborted","timestamp":1642887461067,"user_tz":360,"elapsed":10,"user":{"displayName":"Minh Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05642437067105785776"}}},"outputs":[],"source":["df_covid['abstract'].describe(include='all')"]},{"cell_type":"markdown","metadata":{"id":"YxRp-Gsotq9F"},"source":["# Data Pre-processing-Handle Possible Duplicates"]},{"cell_type":"markdown","metadata":{"id":"-v73U72Btq9F"},"source":["When we look at the unique values above, we can see that there are duplicates. It may have caused because of author submiting the article to multiple journals. Let's remove the duplicats from our dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ojv8kOj1tq9F","executionInfo":{"status":"aborted","timestamp":1642887461067,"user_tz":360,"elapsed":10,"user":{"displayName":"Minh Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05642437067105785776"}}},"outputs":[],"source":["df_covid.dropna(inplace=True)\n","df_covid = df_covid[df_covid.abstract != ''] #Remove rows which are missing abstracts\n","df_covid = df_covid[df_covid.body_text != ''] #Remove rows which are missing body_text\n","df_covid.drop_duplicates(['abstract', 'body_text'], inplace=True) # remove duplicate rows having same abstract and body_text\n","df_covid['abstract'].describe(include='all')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kLLvxB8dtq9G","executionInfo":{"status":"aborted","timestamp":1642887461068,"user_tz":360,"elapsed":11,"user":{"displayName":"Minh Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05642437067105785776"}}},"outputs":[],"source":["df_covid['body_text'].describe(include='all')"]},{"cell_type":"markdown","metadata":{"id":"2IwLXzGOtq9G"},"source":["It looks like we didn't have duplicates. Instead, it was articles without Abstracts."]},{"cell_type":"markdown","metadata":{"id":"fAUiIlMftq9G"},"source":["# Take a Look at the Data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p9BPoHcVtq9G","executionInfo":{"status":"aborted","timestamp":1642887461068,"user_tz":360,"elapsed":11,"user":{"displayName":"Minh Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05642437067105785776"}}},"outputs":[],"source":["df_covid.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5VT5euQtq9G","executionInfo":{"status":"aborted","timestamp":1642887461068,"user_tz":360,"elapsed":11,"user":{"displayName":"Minh Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05642437067105785776"}}},"outputs":[],"source":["df_covid.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BK1xpiMZtq9G","executionInfo":{"status":"aborted","timestamp":1642887461069,"user_tz":360,"elapsed":12,"user":{"displayName":"Minh Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05642437067105785776"}}},"outputs":[],"source":["df_covid.describe()"]},{"cell_type":"markdown","metadata":{"id":"4H73GXFRtq9G"},"source":["Now that we have our dataset loaded, we need to clean-up the text to improve any clustering or classification efforts. First, let's drop Null vales:"]},{"cell_type":"markdown","metadata":{"id":"grBLeVMxtq9G"},"source":["Limit number of articles to speed up computation:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kXDCzSTVtq9G","executionInfo":{"status":"aborted","timestamp":1642887461069,"user_tz":360,"elapsed":11,"user":{"displayName":"Minh Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05642437067105785776"}}},"outputs":[],"source":["# TA: Feel free to decrease this number if you think your system is getting stuck\n","df_covid = df_covid.head(5523)"]},{"cell_type":"markdown","metadata":{"id":"4AvI8noDtq9H"},"source":["Now let's remove punctuation from each text:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WXWcC0N1tq9H","executionInfo":{"status":"aborted","timestamp":1642887461069,"user_tz":360,"elapsed":11,"user":{"displayName":"Minh Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05642437067105785776"}}},"outputs":[],"source":["import re\n","\n","df_covid['body_text'] = df_covid['body_text'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n","df_covid['abstract'] = df_covid['abstract'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))"]},{"cell_type":"markdown","metadata":{"id":"anWf8Z-gtq9H"},"source":["Convert each text to lower case:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"im2DrJRjtq9H","executionInfo":{"status":"aborted","timestamp":1642887461069,"user_tz":360,"elapsed":11,"user":{"displayName":"Minh Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05642437067105785776"}}},"outputs":[],"source":["def lower_case(input_str):\n","    input_str = input_str.lower()\n","    return input_str\n","\n","df_covid['body_text'] = df_covid['body_text'].apply(lambda x: lower_case(x))\n","df_covid['abstract'] = df_covid['abstract'].apply(lambda x: lower_case(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"flGOL-eptq9H","executionInfo":{"status":"aborted","timestamp":1642887461070,"user_tz":360,"elapsed":12,"user":{"displayName":"Minh Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05642437067105785776"}}},"outputs":[],"source":["df_covid.head(4)"]},{"cell_type":"markdown","metadata":{"id":"DsJL3ZCftq9H"},"source":["Now that we have the text cleaned up, we can create our features vector which can be fed into a clustering or dimensionality reduction algorithm. For our first try, we will focus on the text on the body of the articles. Let's grab that:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ncV8LSZtq9H","executionInfo":{"status":"aborted","timestamp":1642887461070,"user_tz":360,"elapsed":12,"user":{"displayName":"Minh Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05642437067105785776"}}},"outputs":[],"source":["text = df_covid.drop([\"paper_id\", \"abstract\", \"abstract_word_count\", \"body_word_count\", \"authors\", \"title\", \"journal\", \"abstract_summary\"], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"39QGqomWtq9I","executionInfo":{"status":"aborted","timestamp":1642887461070,"user_tz":360,"elapsed":12,"user":{"displayName":"Minh Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05642437067105785776"}}},"outputs":[],"source":["text.head(5)"]},{"cell_type":"markdown","metadata":{"id":"Db26n2YUtq9I"},"source":["Let's transform 1D DataFrame into 1D list where each index is an article (instance), so that we can work with words from each instance:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jYuNZiHNtq9I","executionInfo":{"status":"aborted","timestamp":1642887461071,"user_tz":360,"elapsed":13,"user":{"displayName":"Minh Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05642437067105785776"}}},"outputs":[],"source":["text_arr = text.stack().tolist()\n","len(text_arr)"]},{"cell_type":"markdown","metadata":{"id":"bJYywYFXtq9I"},"source":["# 2-Grams"]},{"cell_type":"markdown","metadata":{"id":"kmksZaZztq9I"},"source":["Let's create 2D list, where each row is instance and each column is a word. Meaning, we will separate each instance into words:  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pnoFX7FTtq9I","executionInfo":{"status":"aborted","timestamp":1642887461071,"user_tz":360,"elapsed":13,"user":{"displayName":"Minh Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05642437067105785776"}}},"outputs":[],"source":["words = []\n","for ii in range(0,len(text)):\n","    words.append(str(text.iloc[ii]['body_text']).split(\" \"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"slaAP0lCtq9I","executionInfo":{"status":"aborted","timestamp":1642887461072,"user_tz":360,"elapsed":13,"user":{"displayName":"Minh Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05642437067105785776"}}},"outputs":[],"source":["print(words[0][:20])"]},{"cell_type":"markdown","metadata":{"id":"CPBWlOCMtq9I"},"source":["What we want now is n-grams from the words where n=2 (2-gram). We will still have 2D array where each row is an instance; however, each index in that row going to be a 2-gram:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HBxkYk7qtq9J"},"outputs":[],"source":["n_gram_all = []\n","\n","for word in words:\n","    # get n-grams for the instance\n","    n_gram = []\n","    for i in range(len(word)-2+1):\n","        n_gram.append(\"\".join(word[i:i+2]))\n","    n_gram_all.append(n_gram)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qtvjGEu4tq9J"},"outputs":[],"source":["n_gram_all[0][:10]"]},{"cell_type":"markdown","metadata":{"id":"J6yZvFOhtq9J"},"source":["# Vectorize with HashingVectorizer"]},{"cell_type":"markdown","metadata":{"id":"lk0fv0Ebtq9J"},"source":["Now we will use HashVectorizer to create the features vector X. For now, let's limit the feature size to 2**12(4096) to speed up the computation. We might need to increase this later to reduce the collusions and improve the accuracy:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TpfIwYK6tq9J"},"outputs":[],"source":["from sklearn.feature_extraction.text import HashingVectorizer\n","\n","# hash vectorizer instance\n","hvec = HashingVectorizer(lowercase=False, analyzer=lambda l:l, n_features=2**12)\n","\n","# features matrix X\n","X = hvec.fit_transform(n_gram_all)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T4OIdrZgtq9J"},"outputs":[],"source":["X.shape"]},{"cell_type":"markdown","metadata":{"id":"70PnldhHtq9J"},"source":["#### Separete Training and Test Set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0j1jKwjItq9J"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# test set size of 20% of the data and the random seed 42 <3\n","X_train, X_test = train_test_split(X.toarray(), test_size=0.2, random_state=42)\n","\n","print(\"X_train size:\", len(X_train))\n","print(\"X_test size:\", len(X_test), \"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"U_RKbtxctq9K"},"source":["# Dimensionality Reduction with t-SNE\n","Using t-SNE we can reduce our high dimensional features vector into 2 dimensional plane. In the process, t-SNE will keep similar instances together while trying to push different instances far from each other. Resulting 2-D plane can be useful to see which articles cluster near each other:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JQJh0A2Ftq9K"},"outputs":[],"source":["# Following cell will take 15-20 minutes to run\n","\n","from sklearn.manifold import TSNE\n","\n","tsne = TSNE(verbose=1, perplexity=5)\n","X_embedded = tsne.fit_transform(X_train)"]},{"cell_type":"markdown","metadata":{"id":"DeB5J_cUtq9K"},"source":["Let's plot the result:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w7JtBDdLtq9K"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","import seaborn as sns\n","\n","# sns settings\n","sns.set(rc={'figure.figsize':(15,15)})\n","\n","# colors\n","palette = sns.color_palette(\"bright\", 1)\n","\n","# plot\n","sns.scatterplot(X_embedded[:,0], X_embedded[:,1], palette=palette)\n","\n","plt.title(\"t-SNE Covid-19 Articles\")\n","# plt.savefig(\"plots/t-sne_covid19.png\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ZU2_NHmDtq9K"},"source":["We can clearly see few clusters forming. This may be a good sign that we are able to cluster similar articles together using 2-grams and HashVectorizer with 2**10 features. However, without labels it is difficult to see the clusters. For now, it looks like a blob of data... Let's try if we can use K-Means to generate our labels. We can later use this information to produce a scatterplot with labels to verify the clusters."]},{"cell_type":"markdown","metadata":{"id":"Ivk0GhxBtq9K"},"source":["# Unsupervised Learning: Clustering with K-Means"]},{"cell_type":"markdown","metadata":{"id":"pBU3UVbxtq9K"},"source":["Using K-means we will get the labels we need. For now, we will create 10 clusters. I am choosing this arbitrarily. We can change this later."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3crIoLfRtq9K"},"outputs":[],"source":["# Following cell will take 4-5 minutes to run\n","\n","from sklearn.cluster import KMeans\n","\n","k = 10\n","kmeans = KMeans(n_clusters=k, n_jobs=-1, verbose=10)\n","y_pred = kmeans.fit_predict(X_train)"]},{"cell_type":"markdown","metadata":{"id":"oIKfJ7Qptq9L"},"source":["Labels for the training set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M4J8nBlmtq9L"},"outputs":[],"source":["y_train = y_pred"]},{"cell_type":"markdown","metadata":{"id":"Op0qf5VAtq9L"},"source":["Labels for the test set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T6qxTynFtq9L"},"outputs":[],"source":["y_test = kmeans.predict(X_test)"]},{"cell_type":"markdown","metadata":{"id":"6N_73QsAtq9L"},"source":["Now that we have the labels, let's plot the t-SNE. scatterplot again and see if we have any obvious clusters:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hPShVwjgtq9L"},"outputs":[],"source":["# sns settings\n","sns.set(rc={'figure.figsize':(15,15)})\n","\n","# colors\n","palette = sns.color_palette(\"bright\", len(set(y_pred)))\n","\n","# plot\n","sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y_pred, legend='full', palette=palette)\n","plt.title(\"t-SNE Covid-19 Articles - Clustered\")\n","# plt.savefig(\"plots/t-sne_covid19_label.png\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"52hpTJ4Stq9L"},"source":["That looks pretty promising. It can be seen that articles from the same cluster are near each other, forming groups. There are still overlaps. So we will have to see if we can improve this by changing the cluster size, using another clustering algorithm, or different feature size. We can also consider not using 2-grams, or HashVectorizer. We can try 3-grams, 4-grams, or plain text as our instances and vectorize them using either HashVectorizer, Tf-idfVectorizer, or Burrows Wheeler Transform Distance. <br>\n","\n","Before we try another method for clustering, we want to see how well it will classify using the labels we just created using K-Means."]},{"cell_type":"markdown","metadata":{"id":"wBnCe4B_tq9L"},"source":["# Classify"]},{"cell_type":"markdown","metadata":{"id":"Iuseh33Htq9M"},"source":["### Helper Function:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w7pkmVK6tq9M"},"outputs":[],"source":["# function to print out classification model report\n","def classification_report(model_name, test, pred):\n","    from sklearn.metrics import precision_score, recall_score\n","    from sklearn.metrics import accuracy_score\n","    from sklearn.metrics import f1_score\n","    \n","    print(model_name, \":\\n\")\n","    print(\"Accuracy Score: \", '{:,.3f}'.format(float(accuracy_score(test, pred)) * 100), \"%\")\n","    print(\"     Precision: \", '{:,.3f}'.format(float(precision_score(test, pred, average='micro')) * 100), \"%\")\n","    print(\"        Recall: \", '{:,.3f}'.format(float(recall_score(test, pred, average='micro')) * 100), \"%\")\n","    print(\"      F1 score: \", '{:,.3f}'.format(float(f1_score(test, pred, average='micro')) * 100), \"%\")"]},{"cell_type":"markdown","metadata":{"id":"os2m02gEtq9M"},"source":["### Random Forest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I_kEU71Ktq9M"},"outputs":[],"source":["from sklearn.model_selection import cross_val_score\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# random forest classifier instance\n","forest_clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n","\n","# cross validation on the training set \n","forest_scores = cross_val_score(forest_clf, X_train, y_train, cv=3, n_jobs=-1)\n","\n","# print out the mean of the cross validation scores\n","print(\"Accuracy: \", '{:,.3f}'.format(float(forest_scores.mean()) * 100), \"%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uXlP6PPUtq9M"},"outputs":[],"source":["from sklearn.model_selection import cross_val_predict\n","from sklearn.metrics import precision_score, recall_score\n","\n","# cross validate predict on the training set\n","forest_train_pred = cross_val_predict(forest_clf, X_train, y_train, cv=3, n_jobs=-1)\n","\n","# print precision and recall scores\n","print(\"Precision: \", '{:,.3f}'.format(float(precision_score(y_train, forest_train_pred, average='macro')) * 100), \"%\")\n","print(\"   Recall: \", '{:,.3f}'.format(float(recall_score(y_train, forest_train_pred, average='macro')) * 100), \"%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kjNgNSlitq9M"},"outputs":[],"source":["# first train the model\n","forest_clf.fit(X_train, y_train)\n","\n","# make predictions on the test set\n","forest_pred = forest_clf.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3xxzQftZtq9M"},"outputs":[],"source":["# print out the classification report\n","classification_report(\"Random Forest Classifier Report (Test Set)\", y_test, forest_pred)"]},{"cell_type":"markdown","metadata":{"id":"uwao3zTStq9M"},"source":["It looks like it doesn't overfit, which is good news. But results can be better than ~70-80%."]},{"cell_type":"markdown","metadata":{"id":"nOQnTnXntq9N"},"source":["# Vectorize Using Tf-idf with Plain Text\n","Let's see if we will be able to get better clusters using plain text as instances rather than 2-grams and vectorize it using Tf-idf. Last time we separated the dataset into test and training sets because we wanted to do classification with the labels we got through clustering. This time we will just use the all dataset because the goal is to cluster all literature."]},{"cell_type":"markdown","metadata":{"id":"Wk9x_7Putq9N"},"source":["### Vectorize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FSzOJDJitq9N"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vectorizer = TfidfVectorizer(max_features=2**12)\n","X = vectorizer.fit_transform(df_covid['body_text'].values)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hCMRdHTztq9N"},"outputs":[],"source":["X.shape"]},{"cell_type":"markdown","metadata":{"id":"7dPcDQAutq9N"},"source":["# MiniBatchKMeans with Plain text and Tf-idf"]},{"cell_type":"markdown","metadata":{"id":"zhF_lcKVtq9N"},"source":["Again, let's try to get our labels. We will choose 10 clusters again. This time, we will use MiniBatchKMeans as it is faster with more data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vOOluY2Ntq9N"},"outputs":[],"source":["from sklearn.cluster import MiniBatchKMeans\n","\n","k = 10\n","kmeans = MiniBatchKMeans(n_clusters=k)\n","y_pred = kmeans.fit_predict(X)"]},{"cell_type":"markdown","metadata":{"id":"83-PMECZtq9N"},"source":["Get the labels:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p7nd2bgDtq9O"},"outputs":[],"source":["y = y_pred"]},{"cell_type":"markdown","metadata":{"id":"E0j6h3gftq9O"},"source":["# Dimensionality Reduction with t-SNE (Plain text and Tf-idf)"]},{"cell_type":"markdown","metadata":{"id":"v52-HLz4tq9O"},"source":["Let's reduce the dimensionality using t-SNE again:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9QlvIPZUtq9O"},"outputs":[],"source":["from sklearn.manifold import TSNE\n","\n","tsne = TSNE(verbose=1)\n","X_embedded = tsne.fit_transform(X.toarray())"]},{"cell_type":"markdown","metadata":{"id":"ThFE7aegtq9O"},"source":["### Plot t-SNE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c7tyk44Qtq9O"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","import seaborn as sns\n","\n","# sns settings\n","sns.set(rc={'figure.figsize':(15,15)})\n","\n","# colors\n","palette = sns.color_palette(\"bright\", len(set(y)))\n","\n","# plot\n","sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y, legend='full', palette=palette)\n","plt.title(\"t-SNE Covid-19 Articles - Clustered(K-Means) - Tf-idf with Plain Text\")\n","# plt.savefig(\"plots/t-sne_covid19_label_TFID.png\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"6LGwsJPRtq9O"},"source":["This time we are able to see the clusters more clearly. There are clusters that further apart from each other. I can also start to see that there is possibly more than 10 clusters we need to identify using k-means."]},{"cell_type":"markdown","metadata":{"id":"D8E8CYThtq9O"},"source":["# Dimensionality Reduction with PCA (Plain text and Tf-idf)"]},{"cell_type":"markdown","metadata":{"id":"RyID3Iaktq9O"},"source":["t-SNE doesn't scale well. This is why run-time of this Notebook is about 40 minutes to 1 hour with an average computer. Let's try to see if we dan achive good results with PCA as it scales very well with larger datasets and dimensions:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BQt6cQI5tq9P"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","pca = PCA(n_components=3)\n","pca_result = pca.fit_transform(X.toarray())"]},{"cell_type":"markdown","metadata":{"id":"mJS_qc4xtq9P"},"source":["### Plot PCA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gh5cC4JStq9P"},"outputs":[],"source":["# sns settings\n","sns.set(rc={'figure.figsize':(15,15)})\n","\n","# colors\n","palette = sns.color_palette(\"bright\", len(set(y)))\n","\n","# plot\n","sns.scatterplot(pca_result[:,0], pca_result[:,1], hue=y, legend='full', palette=palette)\n","plt.title(\"PCA Covid-19 Articles - Clustered (K-Means) - Tf-idf with Plain Text\")\n","# plt.savefig(\"plots/pca_covid19_label_TFID.png\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"0WRzhC0ztq9P"},"source":["Sometimes it may be easier to see the results in a 3 dimensional plot. So let's try to do that:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rMmwOPiYtq9P"},"outputs":[],"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","ax = plt.figure(figsize=(16,10)).gca(projection='3d')\n","ax.scatter(\n","    xs=pca_result[:,0], \n","    ys=pca_result[:,1], \n","    zs=pca_result[:,2], \n","    c=y, \n","    cmap='tab10'\n",")\n","ax.set_xlabel('pca-one')\n","ax.set_ylabel('pca-two')\n","ax.set_zlabel('pca-three')\n","plt.title(\"PCA Covid-19 Articles (3D) - Clustered (K-Means) - Tf-idf with Plain Text\")\n","# plt.savefig(\"plots/pca_covid19_label_TFID_3d.png\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"JnwfisNftq9P"},"source":["# More Clusters?\n","On our previous plot we could see that there is more clusters than only 10. Let's try to label them:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k7uYs6xItq9P"},"outputs":[],"source":["from sklearn.cluster import MiniBatchKMeans\n","\n","k = 20\n","kmeans = MiniBatchKMeans(n_clusters=k)\n","y_pred = kmeans.fit_predict(X)\n","y = y_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YIxcGscHtq9P"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","import seaborn as sns\n","import random \n","\n","# sns settings\n","sns.set(rc={'figure.figsize':(15,15)})\n","\n","# let's shuffle the list so distinct colors stay next to each other\n","palette = sns.hls_palette(20, l=.4, s=.9)\n","random.shuffle(palette)\n","\n","# plot\n","sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y, legend='full', palette=palette)\n","plt.title(\"t-SNE Covid-19 Articles - Clustered(K-Means) - Tf-idf with Plain Text\")\n","# plt.savefig(\"plots/t-sne_covid19_20label_TFID.png\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Ug6k-TDHtq9Q"},"source":["It would be helpful if we have a demo tool that can be used to see what articles are identified as similar using our Clustering and Dimensionality Reduction, right? Let's put together a interactive scatter plot of t-SNE to do that."]},{"cell_type":"markdown","metadata":{"id":"-FqhDOlZtq9Q"},"source":["# Interactive t-SNE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hEp8Qlfntq9Q"},"outputs":[],"source":["from bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper, CustomJS\n","from bokeh.palettes import Category20\n","from bokeh.transform import linear_cmap\n","from bokeh.io import output_file, show\n","from bokeh.transform import transform\n","from bokeh.io import output_notebook\n","from bokeh.plotting import figure\n","from bokeh.layouts import column\n","from bokeh.models import RadioButtonGroup\n","from bokeh.models import TextInput\n","from bokeh.layouts import gridplot\n","from bokeh.models import Div\n","from bokeh.models import Paragraph\n","from bokeh.layouts import column, widgetbox\n","\n","output_notebook()\n","y_labels = y_pred\n","\n","# data sources\n","source = ColumnDataSource(data=dict(\n","    x= X_embedded[:,0], \n","    y= X_embedded[:,1],\n","    x_backup = X_embedded[:,0],\n","    y_backup = X_embedded[:,1],\n","    desc= y_labels, \n","    titles= df_covid['title'],\n","    authors = df_covid['authors'],\n","    journal = df_covid['journal'],\n","    abstract = df_covid['abstract_summary'],\n","    labels = [\"C-\" + str(x) for x in y_labels]\n","    ))\n","\n","# hover over information\n","hover = HoverTool(tooltips=[\n","    (\"Title\", \"@titles{safe}\"),\n","    (\"Author(s)\", \"@authors\"),\n","    (\"Journal\", \"@journal\"),\n","    (\"Abstract\", \"@abstract{safe}\"),\n","],\n","                 point_policy=\"follow_mouse\")\n","\n","# map colors\n","mapper = linear_cmap(field_name='desc', \n","                     palette=Category20[20],\n","                     low=min(y_labels) ,high=max(y_labels))\n","\n","# prepare the figure\n","p = figure(plot_width=800, plot_height=800, \n","           tools=[hover, 'pan', 'wheel_zoom', 'box_zoom', 'reset'], \n","           title=\"t-SNE Covid-19 Articles, Clustered(K-Means), Tf-idf with Plain Text\", \n","           toolbar_location=\"right\")\n","\n","# plot\n","p.scatter('x', 'y', size=5, \n","          source=source,\n","          fill_color=mapper,\n","          line_alpha=0.3,\n","          line_color=\"black\",\n","          legend = 'labels')\n","\n","# add callback to control \n","callback = CustomJS(args=dict(p=p, source=source), code=\"\"\"\n","            \n","            var radio_value = cb_obj.active;\n","            var data = source.data; \n","            \n","            x = data['x'];\n","            y = data['y'];\n","            \n","            x_backup = data['x_backup'];\n","            y_backup = data['y_backup'];\n","            \n","            labels = data['desc'];\n","            \n","            if (radio_value == '20') {\n","                for (i = 0; i < x.length; i++) {\n","                    x[i] = x_backup[i];\n","                    y[i] = y_backup[i];\n","                }\n","            }\n","            else {\n","                for (i = 0; i < x.length; i++) {\n","                    if(labels[i] == radio_value) {\n","                        x[i] = x_backup[i];\n","                        y[i] = y_backup[i];\n","                    } else {\n","                        x[i] = undefined;\n","                        y[i] = undefined;\n","                    }\n","                }\n","            }\n","\n","\n","        source.change.emit();\n","        \"\"\")\n","\n","# callback for searchbar\n","keyword_callback = CustomJS(args=dict(p=p, source=source), code=\"\"\"\n","            \n","            var text_value = cb_obj.value;\n","            var data = source.data; \n","            \n","            x = data['x'];\n","            y = data['y'];\n","            \n","            x_backup = data['x_backup'];\n","            y_backup = data['y_backup'];\n","            \n","            abstract = data['abstract'];\n","            titles = data['titles'];\n","            authors = data['authors'];\n","            journal = data['journal'];\n","\n","            for (i = 0; i < x.length; i++) {\n","                if(abstract[i].includes(text_value) || \n","                   titles[i].includes(text_value) || \n","                   authors[i].includes(text_value) || \n","                   journal[i].includes(text_value)) {\n","                    x[i] = x_backup[i];\n","                    y[i] = y_backup[i];\n","                } else {\n","                    x[i] = undefined;\n","                    y[i] = undefined;\n","                }\n","            }\n","            \n","\n","\n","        source.change.emit();\n","        \"\"\")\n","\n","# option\n","option = RadioButtonGroup(labels=[\"C-0\", \"C-1\", \"C-2\",\n","                                  \"C-3\", \"C-4\", \"C-5\",\n","                                  \"C-6\", \"C-7\", \"C-8\",\n","                                  \"C-9\", \"C-10\", \"C-11\",\n","                                  \"C-12\", \"C-13\", \"C-14\",\n","                                  \"C-15\", \"C-16\", \"C-17\",\n","                                  \"C-18\", \"C-19\", \"All\"], \n","                          active=20, callback=callback)\n","\n","# search box\n","keyword = TextInput(title=\"Search:\", callback=keyword_callback)\n","\n","#header\n","header = Div(text=\"\"\"<h1>COVID-19 Literature Cluster</h1>\"\"\")\n","\n","# show\n","show(column(header, widgetbox(option, keyword),p))"]},{"cell_type":"markdown","metadata":{"id":"AMZna-3Ltq9Q"},"source":["#### Please see the tools on right top.\n","#### If the text doesn't fit in the screen on the above plot when hover, please try the 'Box Zoom' tool to zoom to the area where the target plot is. This will help the hover message to fit the screen. \n","#### Use the 'Reset' button to revert the zoom."]},{"cell_type":"markdown","metadata":{"id":"j6pswj0ftq9Q"},"source":["This notebook is adaption from the following kaggle notebook https://www.kaggle.com/maksimeren/covid-19-literature-clustering\n","### You can find the full version of the interactive plot here on GitHub: \n","#### https://maksimekin.github.io/COVID19-Literature-Clustering/plots/t-sne_covid-19_interactive.html\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EDFU76xctq9R"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"covid-19-literature-clustering.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}